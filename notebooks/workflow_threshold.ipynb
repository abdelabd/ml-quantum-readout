{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 16:05:03.569252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 16:05:03.673848: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-21 16:05:03.673870: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-21 16:05:04.147445: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-21 16:05:04.147521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-21 16:05:04.147529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Unable to import optimizer(s) from expr_templates.py: No module named 'sympy'\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aelabd/miniforge3/envs/ml4qick-env/lib/python3.8/site-packages/hls4ml/converters/__init__.py:27: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "sys.path.append(\"../training\")\n",
    "import pickle\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import hls4ml \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras import QBatchNormalization\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "# Local imports\n",
    "from save_data import process_data\n",
    "from threshold import ThresholdModel\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_WINDOW = 285\n",
    "END_WINDOW = 385\n",
    "\n",
    "# START_WINDOW = 150\n",
    "# END_WINDOW = 350\n",
    "\n",
    "# START_WINDOW = 150\n",
    "# END_WINDOW = 550\n",
    "\n",
    "# START_WINDOW = 0\n",
    "# END_WINDOW = 770\n",
    "\n",
    "\n",
    "!mkdir -p models\n",
    "\n",
    "DATA_IN_DIR = f'../data/raw-data'\n",
    "DATA_OUT_DIR = f'../data/qick_data/{START_WINDOW}_{END_WINDOW}'\n",
    "MODEL_DIR = f'models/model_{START_WINDOW}_{END_WINDOW}'\n",
    "\n",
    "# convert raw ADC data into npy files \n",
    "if os.path.exists(f'{DATA_OUT_DIR}/X_train.npy') == False:\n",
    "    process_data(\n",
    "        start_window = START_WINDOW, \n",
    "        end_window = END_WINDOW, \n",
    "        data_in = DATA_IN_DIR,\n",
    "        data_out = DATA_OUT_DIR\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\tSize: 909000\n",
      "\tSample Shape: 200\n",
      "\tMean: -28.26868580858086\n",
      "\tStd. Dev.: 1757.307338388309\n",
      "Testing:\n",
      "\tSize: 101000\n",
      "\tSample Shape: 200\n",
      "\tSample Shape: -21.936618267326732\n",
      "\tStd. Dev.: 1755.8698046711418\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X_train_val = np.load(os.path.join(DATA_OUT_DIR, 'X_train.npy'))\n",
    "X_test = np.load(os.path.join(DATA_OUT_DIR, 'X_test.npy'))    \n",
    "y_train_val = np.load(os.path.join(DATA_OUT_DIR, 'y_train.npy'))\n",
    "y_test = np.load(os.path.join(DATA_OUT_DIR, 'y_test.npy'))\n",
    "\n",
    "# y_train_val = one_hot_encode(y_train_val) # 1xN binary --> 2xN one-hot-encoded\n",
    "# y_test = one_hot_encode(y_test)\n",
    "\n",
    "print(\"Training:\")\n",
    "print(\"\\tSize:\", len(X_train_val))\n",
    "print(\"\\tSample Shape:\", len(X_train_val[0]))\n",
    "print(\"\\tMean:\", X_train_val.mean())\n",
    "print(\"\\tStd. Dev.:\", X_train_val.std())\n",
    "\n",
    "print(\"Testing:\")\n",
    "print(\"\\tSize:\", len(X_test))\n",
    "print(\"\\tSample Shape:\", len(X_test[0]))\n",
    "print(\"\\tSample Shape:\", X_test.mean())\n",
    "print(\"\\tStd. Dev.:\", X_test.std())\n",
    "\n",
    "# X_train_val[0] = I, Q timeseries over (END_WINDOW - START_WINDOW) timesteps\n",
    "assert len(X_train_val[0]) == (END_WINDOW-START_WINDOW)*2, \"ERROR: Specified window does not match loaded dataset shape\"\n",
    "assert len(X_test[0]) == (END_WINDOW-START_WINDOW)*2, \"ERROR: Specified window does not match loaded dataset shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "CHECKPOINT_FILENAME = os.path.join(MODEL_DIR, 'threshold_model.json')\n",
    "INPUT_SHAPE = (len(X_train_val[0]),)\n",
    "print(INPUT_SHAPE)\n",
    "\n",
    "model = ThresholdModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tv_shuffler = np.random.permutation(X_train_val.shape[0])\n",
    "\n",
    "X_train_val = X_train_val[tv_shuffler]\n",
    "y_train_val = y_train_val[tv_shuffler]\n",
    "\n",
    "X_train = X_train_val[:int((1-VALIDATION_SPLIT)*X_train_val.shape[0])]\n",
    "y_train = y_train_val[:int((1-VALIDATION_SPLIT)*X_train_val.shape[0])]\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.save(CHECKPOINT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Pruning layers must be removed before saving to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold model  accuracy: 0.8785346534653465\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"Threshold model  accuracy: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold model  accuracy: 0.8785346534653465\n"
     ]
    }
   ],
   "source": [
    "model = ThresholdModel()\n",
    "model.load(CHECKPOINT_FILENAME)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Threshold model  accuracy: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
